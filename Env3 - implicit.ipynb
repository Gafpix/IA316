{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environnement 3 - Implicit Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "\n",
    "Loss normale  \n",
    "Loss prix\n",
    "\n",
    "negative au pif   \n",
    "negatives non positifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_ID = '9G08LOYFU88BJ8GHNRU3'\n",
    "env = 'http://35.180.178.243/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url=env+'reset', params= {'user_id':USER_ID})\n",
    "data = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_users = data['nb_users']\n",
    "nb_items = data['nb_items']\n",
    "state_history = data['state_history']\n",
    "rewards_history = data['rewards_history']\n",
    "action_history = data['action_history']\n",
    "next_state = data['next_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>items</th>\n",
       "      <th>price</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43</td>\n",
       "      <td>16</td>\n",
       "      <td>396.356183</td>\n",
       "      <td>0.996481</td>\n",
       "      <td>0.467585</td>\n",
       "      <td>-0.389859</td>\n",
       "      <td>1.416562</td>\n",
       "      <td>0.428502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>966.247723</td>\n",
       "      <td>-0.486905</td>\n",
       "      <td>0.685810</td>\n",
       "      <td>1.341728</td>\n",
       "      <td>2.511186</td>\n",
       "      <td>1.257132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>57.140884</td>\n",
       "      <td>1.711386</td>\n",
       "      <td>1.125755</td>\n",
       "      <td>0.051663</td>\n",
       "      <td>1.136483</td>\n",
       "      <td>1.182125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72</td>\n",
       "      <td>10</td>\n",
       "      <td>626.453620</td>\n",
       "      <td>0.189570</td>\n",
       "      <td>1.515007</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>2.689529</td>\n",
       "      <td>0.102660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>322.979246</td>\n",
       "      <td>1.143308</td>\n",
       "      <td>0.882586</td>\n",
       "      <td>1.646076</td>\n",
       "      <td>2.820581</td>\n",
       "      <td>0.334421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>576.351700</td>\n",
       "      <td>-0.838260</td>\n",
       "      <td>0.699417</td>\n",
       "      <td>0.502383</td>\n",
       "      <td>0.255565</td>\n",
       "      <td>0.725593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>878.462686</td>\n",
       "      <td>0.208300</td>\n",
       "      <td>2.107052</td>\n",
       "      <td>2.271528</td>\n",
       "      <td>0.445847</td>\n",
       "      <td>1.507063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>72</td>\n",
       "      <td>7</td>\n",
       "      <td>905.177536</td>\n",
       "      <td>0.189570</td>\n",
       "      <td>1.515007</td>\n",
       "      <td>0.163377</td>\n",
       "      <td>1.481198</td>\n",
       "      <td>0.714706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>35</td>\n",
       "      <td>27</td>\n",
       "      <td>464.832108</td>\n",
       "      <td>2.162890</td>\n",
       "      <td>-0.494537</td>\n",
       "      <td>1.586978</td>\n",
       "      <td>2.084642</td>\n",
       "      <td>1.615104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>81</td>\n",
       "      <td>21</td>\n",
       "      <td>966.247723</td>\n",
       "      <td>0.289245</td>\n",
       "      <td>-0.718787</td>\n",
       "      <td>1.341728</td>\n",
       "      <td>2.511186</td>\n",
       "      <td>1.274096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>731.683263</td>\n",
       "      <td>-0.096033</td>\n",
       "      <td>1.677738</td>\n",
       "      <td>0.482633</td>\n",
       "      <td>0.005676</td>\n",
       "      <td>1.076850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>58</td>\n",
       "      <td>3</td>\n",
       "      <td>731.683263</td>\n",
       "      <td>0.319708</td>\n",
       "      <td>0.599370</td>\n",
       "      <td>0.482633</td>\n",
       "      <td>0.005676</td>\n",
       "      <td>1.811669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>54</td>\n",
       "      <td>17</td>\n",
       "      <td>648.193161</td>\n",
       "      <td>1.687423</td>\n",
       "      <td>2.678898</td>\n",
       "      <td>1.907225</td>\n",
       "      <td>1.557385</td>\n",
       "      <td>1.937293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>944.568415</td>\n",
       "      <td>0.505697</td>\n",
       "      <td>1.190335</td>\n",
       "      <td>1.653496</td>\n",
       "      <td>1.025040</td>\n",
       "      <td>0.910948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>878.462686</td>\n",
       "      <td>0.950543</td>\n",
       "      <td>-0.242775</td>\n",
       "      <td>2.271528</td>\n",
       "      <td>0.445847</td>\n",
       "      <td>5.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>396.356183</td>\n",
       "      <td>1.696192</td>\n",
       "      <td>1.334499</td>\n",
       "      <td>-0.389859</td>\n",
       "      <td>1.416562</td>\n",
       "      <td>-0.237626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "      <td>285.319953</td>\n",
       "      <td>1.696192</td>\n",
       "      <td>1.334499</td>\n",
       "      <td>1.392672</td>\n",
       "      <td>0.293438</td>\n",
       "      <td>0.779830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>102.895442</td>\n",
       "      <td>0.421036</td>\n",
       "      <td>-0.083679</td>\n",
       "      <td>-1.763665</td>\n",
       "      <td>0.977213</td>\n",
       "      <td>1.303371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>322.979246</td>\n",
       "      <td>1.375150</td>\n",
       "      <td>1.953895</td>\n",
       "      <td>1.646076</td>\n",
       "      <td>2.820581</td>\n",
       "      <td>3.849169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>995.760621</td>\n",
       "      <td>1.421994</td>\n",
       "      <td>1.398206</td>\n",
       "      <td>-0.073280</td>\n",
       "      <td>2.308689</td>\n",
       "      <td>0.558810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>905.177536</td>\n",
       "      <td>1.060370</td>\n",
       "      <td>0.767380</td>\n",
       "      <td>0.163377</td>\n",
       "      <td>1.481198</td>\n",
       "      <td>0.105838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>66</td>\n",
       "      <td>11</td>\n",
       "      <td>40.951908</td>\n",
       "      <td>1.361496</td>\n",
       "      <td>2.794540</td>\n",
       "      <td>2.206710</td>\n",
       "      <td>-0.413238</td>\n",
       "      <td>2.525472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>52</td>\n",
       "      <td>5</td>\n",
       "      <td>322.979246</td>\n",
       "      <td>1.919239</td>\n",
       "      <td>0.421224</td>\n",
       "      <td>1.646076</td>\n",
       "      <td>2.820581</td>\n",
       "      <td>1.688925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>449.755003</td>\n",
       "      <td>-0.196308</td>\n",
       "      <td>1.553315</td>\n",
       "      <td>1.862975</td>\n",
       "      <td>1.073021</td>\n",
       "      <td>-0.836815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>549.335469</td>\n",
       "      <td>0.680002</td>\n",
       "      <td>1.891775</td>\n",
       "      <td>0.602406</td>\n",
       "      <td>2.137145</td>\n",
       "      <td>1.933434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>70</td>\n",
       "      <td>29</td>\n",
       "      <td>333.089844</td>\n",
       "      <td>0.979832</td>\n",
       "      <td>1.518316</td>\n",
       "      <td>1.310046</td>\n",
       "      <td>0.420840</td>\n",
       "      <td>0.511413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>731.683263</td>\n",
       "      <td>-0.486905</td>\n",
       "      <td>0.685810</td>\n",
       "      <td>0.482633</td>\n",
       "      <td>0.005676</td>\n",
       "      <td>1.261702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>731.683263</td>\n",
       "      <td>-0.917880</td>\n",
       "      <td>1.981453</td>\n",
       "      <td>0.482633</td>\n",
       "      <td>0.005676</td>\n",
       "      <td>2.100262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>54</td>\n",
       "      <td>5</td>\n",
       "      <td>322.979246</td>\n",
       "      <td>1.687423</td>\n",
       "      <td>2.678898</td>\n",
       "      <td>1.646076</td>\n",
       "      <td>2.820581</td>\n",
       "      <td>2.436796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>67</td>\n",
       "      <td>9</td>\n",
       "      <td>700.568604</td>\n",
       "      <td>2.089680</td>\n",
       "      <td>2.061446</td>\n",
       "      <td>0.367356</td>\n",
       "      <td>-0.389202</td>\n",
       "      <td>1.487917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>40.951908</td>\n",
       "      <td>0.877806</td>\n",
       "      <td>-0.833065</td>\n",
       "      <td>2.206710</td>\n",
       "      <td>-0.413238</td>\n",
       "      <td>1.553044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>43</td>\n",
       "      <td>14</td>\n",
       "      <td>174.626197</td>\n",
       "      <td>0.996481</td>\n",
       "      <td>0.467585</td>\n",
       "      <td>1.085368</td>\n",
       "      <td>0.638969</td>\n",
       "      <td>0.842921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>86</td>\n",
       "      <td>20</td>\n",
       "      <td>90.829274</td>\n",
       "      <td>0.385418</td>\n",
       "      <td>0.643298</td>\n",
       "      <td>1.311301</td>\n",
       "      <td>2.177311</td>\n",
       "      <td>1.965200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>48</td>\n",
       "      <td>13</td>\n",
       "      <td>944.568415</td>\n",
       "      <td>0.234519</td>\n",
       "      <td>1.883455</td>\n",
       "      <td>1.653496</td>\n",
       "      <td>1.025040</td>\n",
       "      <td>0.346594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>70</td>\n",
       "      <td>15</td>\n",
       "      <td>549.335469</td>\n",
       "      <td>0.979832</td>\n",
       "      <td>1.518316</td>\n",
       "      <td>0.602406</td>\n",
       "      <td>2.137145</td>\n",
       "      <td>2.253816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>50</td>\n",
       "      <td>28</td>\n",
       "      <td>285.319953</td>\n",
       "      <td>1.414619</td>\n",
       "      <td>0.406801</td>\n",
       "      <td>1.392672</td>\n",
       "      <td>0.293438</td>\n",
       "      <td>0.897591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>700.568604</td>\n",
       "      <td>-0.917880</td>\n",
       "      <td>1.981453</td>\n",
       "      <td>0.367356</td>\n",
       "      <td>-0.389202</td>\n",
       "      <td>1.537466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>78</td>\n",
       "      <td>26</td>\n",
       "      <td>878.462686</td>\n",
       "      <td>0.427876</td>\n",
       "      <td>-0.059727</td>\n",
       "      <td>2.271528</td>\n",
       "      <td>0.445847</td>\n",
       "      <td>1.068261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>135.447477</td>\n",
       "      <td>-0.585506</td>\n",
       "      <td>0.661162</td>\n",
       "      <td>4.277765</td>\n",
       "      <td>-0.275762</td>\n",
       "      <td>1.340649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>72</td>\n",
       "      <td>25</td>\n",
       "      <td>612.604911</td>\n",
       "      <td>0.189570</td>\n",
       "      <td>1.515007</td>\n",
       "      <td>1.945010</td>\n",
       "      <td>0.987992</td>\n",
       "      <td>0.958061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>56</td>\n",
       "      <td>9</td>\n",
       "      <td>700.568604</td>\n",
       "      <td>1.534208</td>\n",
       "      <td>-0.683136</td>\n",
       "      <td>0.367356</td>\n",
       "      <td>-0.389202</td>\n",
       "      <td>0.100271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>96</td>\n",
       "      <td>28</td>\n",
       "      <td>285.319953</td>\n",
       "      <td>0.981589</td>\n",
       "      <td>2.090463</td>\n",
       "      <td>1.392672</td>\n",
       "      <td>0.293438</td>\n",
       "      <td>0.864136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>174.626197</td>\n",
       "      <td>0.505697</td>\n",
       "      <td>1.190335</td>\n",
       "      <td>1.085368</td>\n",
       "      <td>0.638969</td>\n",
       "      <td>0.502520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>67</td>\n",
       "      <td>20</td>\n",
       "      <td>90.829274</td>\n",
       "      <td>2.089680</td>\n",
       "      <td>2.061446</td>\n",
       "      <td>1.311301</td>\n",
       "      <td>2.177311</td>\n",
       "      <td>1.606596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>57.140884</td>\n",
       "      <td>1.796387</td>\n",
       "      <td>1.458060</td>\n",
       "      <td>0.051663</td>\n",
       "      <td>1.136483</td>\n",
       "      <td>1.215580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>86</td>\n",
       "      <td>14</td>\n",
       "      <td>174.626197</td>\n",
       "      <td>0.385418</td>\n",
       "      <td>0.643298</td>\n",
       "      <td>1.085368</td>\n",
       "      <td>0.638969</td>\n",
       "      <td>0.387245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>49</td>\n",
       "      <td>11</td>\n",
       "      <td>40.951908</td>\n",
       "      <td>1.421994</td>\n",
       "      <td>1.398206</td>\n",
       "      <td>2.206710</td>\n",
       "      <td>-0.413238</td>\n",
       "      <td>0.423297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>60</td>\n",
       "      <td>29</td>\n",
       "      <td>333.089844</td>\n",
       "      <td>-0.096033</td>\n",
       "      <td>1.677738</td>\n",
       "      <td>1.310046</td>\n",
       "      <td>0.420840</td>\n",
       "      <td>2.958323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>460.772110</td>\n",
       "      <td>1.060370</td>\n",
       "      <td>0.767380</td>\n",
       "      <td>0.981796</td>\n",
       "      <td>0.764697</td>\n",
       "      <td>1.064587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>51</td>\n",
       "      <td>12</td>\n",
       "      <td>460.772110</td>\n",
       "      <td>3.281036</td>\n",
       "      <td>1.778999</td>\n",
       "      <td>0.981796</td>\n",
       "      <td>0.764697</td>\n",
       "      <td>1.386207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>98</td>\n",
       "      <td>3</td>\n",
       "      <td>731.683263</td>\n",
       "      <td>0.954634</td>\n",
       "      <td>1.426984</td>\n",
       "      <td>0.482633</td>\n",
       "      <td>0.005676</td>\n",
       "      <td>-0.418965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>80</td>\n",
       "      <td>27</td>\n",
       "      <td>464.832108</td>\n",
       "      <td>2.162465</td>\n",
       "      <td>0.945985</td>\n",
       "      <td>1.586978</td>\n",
       "      <td>2.084642</td>\n",
       "      <td>1.087175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>576.351700</td>\n",
       "      <td>2.492151</td>\n",
       "      <td>2.201402</td>\n",
       "      <td>0.502383</td>\n",
       "      <td>0.255565</td>\n",
       "      <td>2.261161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>87</td>\n",
       "      <td>15</td>\n",
       "      <td>549.335469</td>\n",
       "      <td>0.900598</td>\n",
       "      <td>-0.379440</td>\n",
       "      <td>0.602406</td>\n",
       "      <td>2.137145</td>\n",
       "      <td>0.619964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "      <td>731.683263</td>\n",
       "      <td>2.291376</td>\n",
       "      <td>1.961324</td>\n",
       "      <td>0.482633</td>\n",
       "      <td>0.005676</td>\n",
       "      <td>1.060709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>94</td>\n",
       "      <td>25</td>\n",
       "      <td>612.604911</td>\n",
       "      <td>0.965137</td>\n",
       "      <td>-1.326437</td>\n",
       "      <td>1.945010</td>\n",
       "      <td>0.987992</td>\n",
       "      <td>1.567216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>34</td>\n",
       "      <td>19</td>\n",
       "      <td>57.140884</td>\n",
       "      <td>2.147475</td>\n",
       "      <td>1.182220</td>\n",
       "      <td>0.051663</td>\n",
       "      <td>1.136483</td>\n",
       "      <td>0.900874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>95</td>\n",
       "      <td>23</td>\n",
       "      <td>282.958628</td>\n",
       "      <td>1.196343</td>\n",
       "      <td>1.578995</td>\n",
       "      <td>1.019945</td>\n",
       "      <td>-0.697800</td>\n",
       "      <td>0.187476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>40.951908</td>\n",
       "      <td>-0.103979</td>\n",
       "      <td>-2.789978</td>\n",
       "      <td>2.206710</td>\n",
       "      <td>-0.413238</td>\n",
       "      <td>1.283011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>41</td>\n",
       "      <td>19</td>\n",
       "      <td>57.140884</td>\n",
       "      <td>1.320337</td>\n",
       "      <td>0.095607</td>\n",
       "      <td>0.051663</td>\n",
       "      <td>1.136483</td>\n",
       "      <td>1.888091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     users  items       price         0         1         2         3  \\\n",
       "0       43     16  396.356183  0.996481  0.467585 -0.389859  1.416562   \n",
       "1        2     21  966.247723 -0.486905  0.685810  1.341728  2.511186   \n",
       "2       13     19   57.140884  1.711386  1.125755  0.051663  1.136483   \n",
       "3       72     10  626.453620  0.189570  1.515007  0.001726  2.689529   \n",
       "4       39      5  322.979246  1.143308  0.882586  1.646076  2.820581   \n",
       "5       22      0  576.351700 -0.838260  0.699417  0.502383  0.255565   \n",
       "6       19     26  878.462686  0.208300  2.107052  2.271528  0.445847   \n",
       "7       72      7  905.177536  0.189570  1.515007  0.163377  1.481198   \n",
       "8       35     27  464.832108  2.162890 -0.494537  1.586978  2.084642   \n",
       "9       81     21  966.247723  0.289245 -0.718787  1.341728  2.511186   \n",
       "10      60      3  731.683263 -0.096033  1.677738  0.482633  0.005676   \n",
       "11      58      3  731.683263  0.319708  0.599370  0.482633  0.005676   \n",
       "12      54     17  648.193161  1.687423  2.678898  1.907225  1.557385   \n",
       "13      15     13  944.568415  0.505697  1.190335  1.653496  1.025040   \n",
       "14       5     26  878.462686  0.950543 -0.242775  2.271528  0.445847   \n",
       "15      24     16  396.356183  1.696192  1.334499 -0.389859  1.416562   \n",
       "16      24     28  285.319953  1.696192  1.334499  1.392672  0.293438   \n",
       "17      61      2  102.895442  0.421036 -0.083679 -1.763665  0.977213   \n",
       "18      38      5  322.979246  1.375150  1.953895  1.646076  2.820581   \n",
       "19      49      1  995.760621  1.421994  1.398206 -0.073280  2.308689   \n",
       "20      12      7  905.177536  1.060370  0.767380  0.163377  1.481198   \n",
       "21      66     11   40.951908  1.361496  2.794540  2.206710 -0.413238   \n",
       "22      52      5  322.979246  1.919239  0.421224  1.646076  2.820581   \n",
       "23      36      4  449.755003 -0.196308  1.553315  1.862975  1.073021   \n",
       "24      23     15  549.335469  0.680002  1.891775  0.602406  2.137145   \n",
       "25      70     29  333.089844  0.979832  1.518316  1.310046  0.420840   \n",
       "26       2      3  731.683263 -0.486905  0.685810  0.482633  0.005676   \n",
       "27      28      3  731.683263 -0.917880  1.981453  0.482633  0.005676   \n",
       "28      54      5  322.979246  1.687423  2.678898  1.646076  2.820581   \n",
       "29      67      9  700.568604  2.089680  2.061446  0.367356 -0.389202   \n",
       "..     ...    ...         ...       ...       ...       ...       ...   \n",
       "170      8     11   40.951908  0.877806 -0.833065  2.206710 -0.413238   \n",
       "171     43     14  174.626197  0.996481  0.467585  1.085368  0.638969   \n",
       "172     86     20   90.829274  0.385418  0.643298  1.311301  2.177311   \n",
       "173     48     13  944.568415  0.234519  1.883455  1.653496  1.025040   \n",
       "174     70     15  549.335469  0.979832  1.518316  0.602406  2.137145   \n",
       "175     50     28  285.319953  1.414619  0.406801  1.392672  0.293438   \n",
       "176     28      9  700.568604 -0.917880  1.981453  0.367356 -0.389202   \n",
       "177     78     26  878.462686  0.427876 -0.059727  2.271528  0.445847   \n",
       "178      0      8  135.447477 -0.585506  0.661162  4.277765 -0.275762   \n",
       "179     72     25  612.604911  0.189570  1.515007  1.945010  0.987992   \n",
       "180     56      9  700.568604  1.534208 -0.683136  0.367356 -0.389202   \n",
       "181     96     28  285.319953  0.981589  2.090463  1.392672  0.293438   \n",
       "182     15     14  174.626197  0.505697  1.190335  1.085368  0.638969   \n",
       "183     67     20   90.829274  2.089680  2.061446  1.311301  2.177311   \n",
       "184     18     19   57.140884  1.796387  1.458060  0.051663  1.136483   \n",
       "185     86     14  174.626197  0.385418  0.643298  1.085368  0.638969   \n",
       "186     49     11   40.951908  1.421994  1.398206  2.206710 -0.413238   \n",
       "187     60     29  333.089844 -0.096033  1.677738  1.310046  0.420840   \n",
       "188     12     12  460.772110  1.060370  0.767380  0.981796  0.764697   \n",
       "189     51     12  460.772110  3.281036  1.778999  0.981796  0.764697   \n",
       "190     98      3  731.683263  0.954634  1.426984  0.482633  0.005676   \n",
       "191     80     27  464.832108  2.162465  0.945985  1.586978  2.084642   \n",
       "192     21      0  576.351700  2.492151  2.201402  0.502383  0.255565   \n",
       "193     87     15  549.335469  0.900598 -0.379440  0.602406  2.137145   \n",
       "194     47      3  731.683263  2.291376  1.961324  0.482633  0.005676   \n",
       "195     94     25  612.604911  0.965137 -1.326437  1.945010  0.987992   \n",
       "196     34     19   57.140884  2.147475  1.182220  0.051663  1.136483   \n",
       "197     95     23  282.958628  1.196343  1.578995  1.019945 -0.697800   \n",
       "198      9     11   40.951908 -0.103979 -2.789978  2.206710 -0.413238   \n",
       "199     41     19   57.140884  1.320337  0.095607  0.051663  1.136483   \n",
       "\n",
       "            4  \n",
       "0    0.428502  \n",
       "1    1.257132  \n",
       "2    1.182125  \n",
       "3    0.102660  \n",
       "4    0.334421  \n",
       "5    0.725593  \n",
       "6    1.507063  \n",
       "7    0.714706  \n",
       "8    1.615104  \n",
       "9    1.274096  \n",
       "10   1.076850  \n",
       "11   1.811669  \n",
       "12   1.937293  \n",
       "13   0.910948  \n",
       "14   5.000018  \n",
       "15  -0.237626  \n",
       "16   0.779830  \n",
       "17   1.303371  \n",
       "18   3.849169  \n",
       "19   0.558810  \n",
       "20   0.105838  \n",
       "21   2.525472  \n",
       "22   1.688925  \n",
       "23  -0.836815  \n",
       "24   1.933434  \n",
       "25   0.511413  \n",
       "26   1.261702  \n",
       "27   2.100262  \n",
       "28   2.436796  \n",
       "29   1.487917  \n",
       "..        ...  \n",
       "170  1.553044  \n",
       "171  0.842921  \n",
       "172  1.965200  \n",
       "173  0.346594  \n",
       "174  2.253816  \n",
       "175  0.897591  \n",
       "176  1.537466  \n",
       "177  1.068261  \n",
       "178  1.340649  \n",
       "179  0.958061  \n",
       "180  0.100271  \n",
       "181  0.864136  \n",
       "182  0.502520  \n",
       "183  1.606596  \n",
       "184  1.215580  \n",
       "185  0.387245  \n",
       "186  0.423297  \n",
       "187  2.958323  \n",
       "188  1.064587  \n",
       "189  1.386207  \n",
       "190 -0.418965  \n",
       "191  1.087175  \n",
       "192  2.261161  \n",
       "193  0.619964  \n",
       "194  1.060709  \n",
       "195  1.567216  \n",
       "196  0.900874  \n",
       "197  0.187476  \n",
       "198  1.283011  \n",
       "199  1.888091  \n",
       "\n",
       "[200 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_data = pd.DataFrame(action_history, columns=['actions'])\n",
    "\n",
    "pos_users = []\n",
    "pos_items = []\n",
    "pos_meta = []\n",
    "\n",
    "for i in range(len(action_history)):\n",
    "    state = state_history[i]\n",
    "    action = action_history[i]\n",
    "    \n",
    "    pos_users.append(state[0][0])\n",
    "    pos_items.append(state[action][1])\n",
    "    pos_meta.append(state[action][2:])\n",
    "    \n",
    "    \n",
    "pos_data['users'] = pos_users\n",
    "pos_data['items'] = pos_items\n",
    "pos_data[['price', '0', '1', '2', '3', '4']] = pd.DataFrame(pos_meta)\n",
    "\n",
    "\n",
    "pos_data = pos_data.drop('actions', axis=1)\n",
    "\n",
    "pos_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Embedding, Flatten, Input, Dense, Dropout\n",
    "from keras.layers import Concatenate, Lambda\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_loss(y_true, y_pred):\n",
    "    \"\"\"Ignore y_true and return the mean of y_pred\n",
    "    \n",
    "    This is a hack to work-around the design of the Keras API that is\n",
    "    not really suited to train networks with a triplet loss by default.\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(y_pred + 0 * y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_comparator_loss(inputs, margin=1.):\n",
    "    \"\"\"Comparator loss for a pair of precomputed similarities\n",
    "    \n",
    "    If the inputs are cosine similarities, they each have range in\n",
    "    (-1, 1), therefore their difference have range in (-2, 2). Using\n",
    "    a margin of 1. can therefore make sense.\n",
    "\n",
    "    If the input similarities are not normalized, it can be beneficial\n",
    "    to use larger values for the margin of the comparator loss.\n",
    "    \"\"\"\n",
    "    positive_pair_sim, negative_pair_sim = inputs\n",
    "    return tf.maximum(negative_pair_sim - positive_pair_sim + margin, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_comparator_loss_improved(inputs, prices, A=0.01, B=100.):\n",
    "    \"\"\"Comparator loss for a pair of precomputed similarities\n",
    "    \n",
    "    If the inputs are cosine similarities, they each have range in\n",
    "    (-1, 1), therefore their difference have range in (-2, 2). Using\n",
    "    a margin of 1. can therefore make sense.\n",
    "\n",
    "    If the input similarities are not normalized, it can be beneficial\n",
    "    to use larger values for the margin of the comparator loss.\n",
    "    \"\"\"\n",
    "    p_sim, n_sim = inputs\n",
    "    p_price, n_price = prices\n",
    "    if p_sim == n_sim:\n",
    "        return tf.maximum((n_price - p_price + B), 0)\n",
    "    \n",
    "    return tf.maximum((A / tf.abs(n_sim - p_sim)) * (n_price - p_price + B) + (n_sim - p_sim), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_interaction_mlp(input_dim, n_hidden=1, hidden_size=64,\n",
    "                         dropout=0, l2_reg=None):\n",
    "    \"\"\"Build the shared multi layer perceptron\"\"\"\n",
    "    mlp = Sequential()\n",
    "    \n",
    "    mlp.add(Dense(hidden_size, input_dim=input_dim,\n",
    "                  activation='relu', kernel_regularizer=l2_reg))\n",
    "    mlp.add(Dropout(dropout))\n",
    "    \n",
    "    for i in range(n_hidden - 1):\n",
    "        mlp.add(Dense(hidden_size, activation='relu',\n",
    "                      W_regularizer=l2_reg))\n",
    "        mlp.add(Dropout(dropout))\n",
    "        \n",
    "    mlp.add(Dense(1, activation='relu', kernel_regularizer=l2_reg))\n",
    "    \n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_models(n_users, n_items, n_meta=6, user_dim=32, item_dim=64,\n",
    "                 n_hidden=1, hidden_size=64, dropout=0, l2_reg=0):\n",
    "    \"\"\"Build models to train a deep triplet network\"\"\"\n",
    "    \n",
    "    user_input = Input((1,), name='user_input')\n",
    "    positive_item_input = Input((1,), name='positive_item_input')\n",
    "    positive_meta_data_input = Input((n_meta,), name='positive_meta_data_input')\n",
    "    negative_item_input = Input((1,), name='negative_item_input')\n",
    "    negative_meta_data_input = Input((n_meta,), name='negative_meta_data_input')\n",
    "    \n",
    "    l2_reg = None if l2_reg == 0 else l2(l2_reg)\n",
    "    \n",
    "    user_layer = Embedding(n_users, user_dim, input_length=1,\n",
    "                           name='user_embedding', embeddings_regularizer=l2_reg)\n",
    "\n",
    "    # The following embedding parameters will be shared to encode both\n",
    "    # the positive and negative items.\n",
    "    item_layer = Embedding(n_items, item_dim, input_length=1,\n",
    "                           name=\"item_embedding\", embeddings_regularizer=l2_reg)\n",
    "\n",
    "    user_embedding = Flatten()(user_layer(user_input))\n",
    "    positive_item_embedding = Flatten()(item_layer(positive_item_input))\n",
    "    negative_item_embedding = Flatten()(item_layer(negative_item_input))\n",
    "\n",
    "\n",
    "    # Similarity computation between embeddings using a MLP similarity\n",
    "    positive_embeddings_pair = Concatenate(name=\"positive_embeddings_pair\")(\n",
    "        [user_embedding, positive_item_embedding, positive_meta_data_input])\n",
    "    positive_embeddings_pair = Dropout(dropout)(positive_embeddings_pair)\n",
    "    negative_embeddings_pair = Concatenate(name=\"negative_embeddings_pair\")(\n",
    "        [user_embedding, negative_item_embedding, negative_meta_data_input])\n",
    "    negative_embeddings_pair = Dropout(dropout)(negative_embeddings_pair)\n",
    "\n",
    "    # Instanciate the shared similarity architecture\n",
    "    interaction_layers = make_interaction_mlp(\n",
    "        user_dim + item_dim + n_meta, n_hidden=n_hidden, hidden_size=hidden_size,\n",
    "        dropout=dropout, l2_reg=l2_reg)\n",
    "\n",
    "    positive_similarity = interaction_layers(positive_embeddings_pair)\n",
    "    negative_similarity = interaction_layers(negative_embeddings_pair)\n",
    "\n",
    "    # The triplet network model, only used for training\n",
    "    triplet_loss = Lambda(margin_comparator_loss, output_shape=(1,),\n",
    "                          name='comparator_loss')(\n",
    "        [positive_similarity, negative_similarity])\n",
    "\n",
    "    deep_triplet_model = Model(inputs=[user_input,\n",
    "                                       positive_item_input,\n",
    "                                       negative_item_input],\n",
    "                               outputs=[triplet_loss])\n",
    "\n",
    "    # The match-score model, only used at inference\n",
    "    deep_match_model = Model(inputs=[user_input, positive_item_input],\n",
    "                             outputs=[positive_similarity])\n",
    "\n",
    "    return deep_match_model, deep_triplet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters = dict(\n",
    "    n_meta=6,\n",
    "    user_dim=32,\n",
    "    item_dim=64,\n",
    "    n_hidden=1,\n",
    "    hidden_size=128,\n",
    "    dropout=0.1,\n",
    "    l2_reg=0\n",
    ")\n",
    "deep_match_model, deep_triplet_model = build_models(n_users, n_items,\n",
    "                                                    **hyper_parameters)\n",
    "\n",
    "\n",
    "deep_triplet_model.compile(loss=identity_loss, optimizer='adam')\n",
    "fake_y = np.ones_like(pos_data_train['user_id'])\n",
    "\n",
    "n_epochs = 15\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    # Sample new negatives to build different triplets at each epoch\n",
    "    triplet_inputs = sample_triplets(pos_data_train, max_item_id,\n",
    "                                     random_seed=i)\n",
    "\n",
    "    # Fit the model incrementally by doing a single pass over the\n",
    "    # sampled triplets.\n",
    "    deep_triplet_model.fit(triplet_inputs, fake_y, shuffle=True,\n",
    "                           batch_size=64, epochs=1)\n",
    "\n",
    "    # Monitor the convergence of the model\n",
    "    test_auc = average_roc_auc(deep_match_model, pos_data_train, pos_data_test)\n",
    "    print(\"Epoch %d/%d: test ROC AUC: %0.4f\"\n",
    "          % (i + 1, n_epochs, test_auc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
