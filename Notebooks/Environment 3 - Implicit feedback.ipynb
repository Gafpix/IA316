{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment 3 - Implicit Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This environment is made to be closer to true real life problems. It works with implicit feedback instead of explicit feedback. This means that instead of knowing the real rating of a product, we just know if the user bought it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_ID = '9G08LOYFU88BJ8GHNRU3'\n",
    "env = 'http://35.180.178.243/'\n",
    "r = requests.get(url=env+'reset', params= {'user_id':USER_ID})\n",
    "data = r.json()\n",
    "nb_users = data['nb_users']\n",
    "nb_items = data['nb_items']\n",
    "state_history = data['state_history']\n",
    "rewards_history = data['rewards_history']\n",
    "action_history = data['action_history']\n",
    "next_state = data['next_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>price</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77</td>\n",
       "      <td>28</td>\n",
       "      <td>28.698722</td>\n",
       "      <td>0.789939</td>\n",
       "      <td>2.507614</td>\n",
       "      <td>0.900374</td>\n",
       "      <td>1.040794</td>\n",
       "      <td>2.902107</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>26</td>\n",
       "      <td>248.907550</td>\n",
       "      <td>-1.330941</td>\n",
       "      <td>1.305624</td>\n",
       "      <td>1.427514</td>\n",
       "      <td>0.968023</td>\n",
       "      <td>0.186144</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76</td>\n",
       "      <td>8</td>\n",
       "      <td>367.506014</td>\n",
       "      <td>1.043017</td>\n",
       "      <td>-0.384232</td>\n",
       "      <td>0.371040</td>\n",
       "      <td>1.340897</td>\n",
       "      <td>1.158571</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>15</td>\n",
       "      <td>61.868779</td>\n",
       "      <td>0.400431</td>\n",
       "      <td>0.665219</td>\n",
       "      <td>0.485358</td>\n",
       "      <td>2.523798</td>\n",
       "      <td>-0.459346</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>7</td>\n",
       "      <td>969.817274</td>\n",
       "      <td>-1.079973</td>\n",
       "      <td>1.119112</td>\n",
       "      <td>1.794775</td>\n",
       "      <td>2.638535</td>\n",
       "      <td>0.742932</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>68</td>\n",
       "      <td>28</td>\n",
       "      <td>28.698722</td>\n",
       "      <td>1.196186</td>\n",
       "      <td>0.559321</td>\n",
       "      <td>0.900374</td>\n",
       "      <td>1.040794</td>\n",
       "      <td>2.364322</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>90</td>\n",
       "      <td>28</td>\n",
       "      <td>28.698722</td>\n",
       "      <td>-1.912106</td>\n",
       "      <td>1.681800</td>\n",
       "      <td>0.900374</td>\n",
       "      <td>1.040794</td>\n",
       "      <td>1.542127</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>74</td>\n",
       "      <td>14</td>\n",
       "      <td>903.194734</td>\n",
       "      <td>1.562109</td>\n",
       "      <td>2.590898</td>\n",
       "      <td>0.679910</td>\n",
       "      <td>1.538540</td>\n",
       "      <td>0.280190</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>993.689301</td>\n",
       "      <td>0.832800</td>\n",
       "      <td>1.486144</td>\n",
       "      <td>1.065739</td>\n",
       "      <td>1.375797</td>\n",
       "      <td>0.177736</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>969.817274</td>\n",
       "      <td>1.732512</td>\n",
       "      <td>0.016619</td>\n",
       "      <td>1.794775</td>\n",
       "      <td>2.638535</td>\n",
       "      <td>1.700727</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>969.817274</td>\n",
       "      <td>-1.330941</td>\n",
       "      <td>1.305624</td>\n",
       "      <td>1.794775</td>\n",
       "      <td>2.638535</td>\n",
       "      <td>-0.002220</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>17.827325</td>\n",
       "      <td>1.394354</td>\n",
       "      <td>2.027665</td>\n",
       "      <td>-0.453177</td>\n",
       "      <td>0.125841</td>\n",
       "      <td>0.835739</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>68</td>\n",
       "      <td>29</td>\n",
       "      <td>254.746559</td>\n",
       "      <td>1.196186</td>\n",
       "      <td>0.559321</td>\n",
       "      <td>0.772554</td>\n",
       "      <td>2.273993</td>\n",
       "      <td>-0.347443</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>17.827325</td>\n",
       "      <td>1.519595</td>\n",
       "      <td>1.152461</td>\n",
       "      <td>-0.453177</td>\n",
       "      <td>0.125841</td>\n",
       "      <td>2.242180</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>100.947533</td>\n",
       "      <td>-0.142724</td>\n",
       "      <td>1.691849</td>\n",
       "      <td>1.325641</td>\n",
       "      <td>2.517639</td>\n",
       "      <td>0.860762</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>38</td>\n",
       "      <td>21</td>\n",
       "      <td>835.214987</td>\n",
       "      <td>0.386115</td>\n",
       "      <td>1.626517</td>\n",
       "      <td>0.731811</td>\n",
       "      <td>0.382787</td>\n",
       "      <td>0.234187</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>100.947533</td>\n",
       "      <td>3.723706</td>\n",
       "      <td>2.145861</td>\n",
       "      <td>1.325641</td>\n",
       "      <td>2.517639</td>\n",
       "      <td>0.784364</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>94</td>\n",
       "      <td>13</td>\n",
       "      <td>922.297143</td>\n",
       "      <td>1.729603</td>\n",
       "      <td>1.883731</td>\n",
       "      <td>-0.190819</td>\n",
       "      <td>1.448809</td>\n",
       "      <td>1.220363</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>410.171349</td>\n",
       "      <td>2.404322</td>\n",
       "      <td>2.625245</td>\n",
       "      <td>1.983686</td>\n",
       "      <td>0.490864</td>\n",
       "      <td>1.008308</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>87</td>\n",
       "      <td>23</td>\n",
       "      <td>672.663258</td>\n",
       "      <td>2.584417</td>\n",
       "      <td>-0.124552</td>\n",
       "      <td>1.190294</td>\n",
       "      <td>0.411279</td>\n",
       "      <td>1.683473</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "      <td>71.688406</td>\n",
       "      <td>-0.065181</td>\n",
       "      <td>-0.448748</td>\n",
       "      <td>1.051414</td>\n",
       "      <td>0.777089</td>\n",
       "      <td>-0.956457</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>100.947533</td>\n",
       "      <td>0.581359</td>\n",
       "      <td>1.570981</td>\n",
       "      <td>1.325641</td>\n",
       "      <td>2.517639</td>\n",
       "      <td>-0.313141</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>85</td>\n",
       "      <td>12</td>\n",
       "      <td>531.135325</td>\n",
       "      <td>-1.446959</td>\n",
       "      <td>0.728564</td>\n",
       "      <td>1.759724</td>\n",
       "      <td>0.253640</td>\n",
       "      <td>-0.335128</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>61</td>\n",
       "      <td>28</td>\n",
       "      <td>28.698722</td>\n",
       "      <td>0.581359</td>\n",
       "      <td>1.570981</td>\n",
       "      <td>0.900374</td>\n",
       "      <td>1.040794</td>\n",
       "      <td>1.114783</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>116.987508</td>\n",
       "      <td>0.856086</td>\n",
       "      <td>-0.159930</td>\n",
       "      <td>1.420047</td>\n",
       "      <td>1.223781</td>\n",
       "      <td>3.580429</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>54</td>\n",
       "      <td>8</td>\n",
       "      <td>367.506014</td>\n",
       "      <td>-0.065181</td>\n",
       "      <td>-0.448748</td>\n",
       "      <td>0.371040</td>\n",
       "      <td>1.340897</td>\n",
       "      <td>0.057889</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>70</td>\n",
       "      <td>25</td>\n",
       "      <td>993.689301</td>\n",
       "      <td>1.415843</td>\n",
       "      <td>0.681872</td>\n",
       "      <td>1.065739</td>\n",
       "      <td>1.375797</td>\n",
       "      <td>1.716005</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13</td>\n",
       "      <td>29</td>\n",
       "      <td>254.746559</td>\n",
       "      <td>-0.607144</td>\n",
       "      <td>0.920339</td>\n",
       "      <td>0.772554</td>\n",
       "      <td>2.273993</td>\n",
       "      <td>0.229921</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>84</td>\n",
       "      <td>28</td>\n",
       "      <td>28.698722</td>\n",
       "      <td>-0.142724</td>\n",
       "      <td>1.691849</td>\n",
       "      <td>0.900374</td>\n",
       "      <td>1.040794</td>\n",
       "      <td>0.159544</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>951.047558</td>\n",
       "      <td>-1.330941</td>\n",
       "      <td>1.305624</td>\n",
       "      <td>2.542116</td>\n",
       "      <td>0.445173</td>\n",
       "      <td>-1.186362</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>116.987508</td>\n",
       "      <td>1.394354</td>\n",
       "      <td>2.027665</td>\n",
       "      <td>1.420047</td>\n",
       "      <td>1.223781</td>\n",
       "      <td>1.180536</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>672.663258</td>\n",
       "      <td>0.443231</td>\n",
       "      <td>0.812970</td>\n",
       "      <td>1.190294</td>\n",
       "      <td>0.411279</td>\n",
       "      <td>0.193846</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>85</td>\n",
       "      <td>18</td>\n",
       "      <td>673.090628</td>\n",
       "      <td>-1.446959</td>\n",
       "      <td>0.728564</td>\n",
       "      <td>1.519853</td>\n",
       "      <td>0.484322</td>\n",
       "      <td>-0.271625</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>43.473372</td>\n",
       "      <td>0.649671</td>\n",
       "      <td>1.368671</td>\n",
       "      <td>2.454132</td>\n",
       "      <td>-0.709874</td>\n",
       "      <td>0.677057</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>673.090628</td>\n",
       "      <td>1.732512</td>\n",
       "      <td>0.016619</td>\n",
       "      <td>1.519853</td>\n",
       "      <td>0.484322</td>\n",
       "      <td>0.685074</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>71</td>\n",
       "      <td>26</td>\n",
       "      <td>248.907550</td>\n",
       "      <td>-0.086325</td>\n",
       "      <td>1.039805</td>\n",
       "      <td>1.427514</td>\n",
       "      <td>0.968023</td>\n",
       "      <td>1.887842</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>83</td>\n",
       "      <td>10</td>\n",
       "      <td>17.827325</td>\n",
       "      <td>2.014021</td>\n",
       "      <td>0.240536</td>\n",
       "      <td>-0.453177</td>\n",
       "      <td>0.125841</td>\n",
       "      <td>1.113439</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>87</td>\n",
       "      <td>5</td>\n",
       "      <td>940.757064</td>\n",
       "      <td>2.584417</td>\n",
       "      <td>-0.124552</td>\n",
       "      <td>0.513526</td>\n",
       "      <td>0.139178</td>\n",
       "      <td>-0.547325</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>92</td>\n",
       "      <td>11</td>\n",
       "      <td>339.098289</td>\n",
       "      <td>0.575016</td>\n",
       "      <td>1.578828</td>\n",
       "      <td>0.521304</td>\n",
       "      <td>3.332927</td>\n",
       "      <td>1.844110</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>70</td>\n",
       "      <td>23</td>\n",
       "      <td>672.663258</td>\n",
       "      <td>1.415843</td>\n",
       "      <td>0.681872</td>\n",
       "      <td>1.190294</td>\n",
       "      <td>0.411279</td>\n",
       "      <td>2.056615</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>39</td>\n",
       "      <td>28</td>\n",
       "      <td>28.698722</td>\n",
       "      <td>-0.593363</td>\n",
       "      <td>2.374605</td>\n",
       "      <td>0.900374</td>\n",
       "      <td>1.040794</td>\n",
       "      <td>2.199441</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>100.947533</td>\n",
       "      <td>-0.677713</td>\n",
       "      <td>1.405754</td>\n",
       "      <td>1.325641</td>\n",
       "      <td>2.517639</td>\n",
       "      <td>-0.736620</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>68</td>\n",
       "      <td>3</td>\n",
       "      <td>71.688406</td>\n",
       "      <td>1.196186</td>\n",
       "      <td>0.559321</td>\n",
       "      <td>1.051414</td>\n",
       "      <td>0.777089</td>\n",
       "      <td>1.333610</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>53</td>\n",
       "      <td>18</td>\n",
       "      <td>673.090628</td>\n",
       "      <td>1.497579</td>\n",
       "      <td>0.342245</td>\n",
       "      <td>1.519853</td>\n",
       "      <td>0.484322</td>\n",
       "      <td>1.191190</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>62</td>\n",
       "      <td>14</td>\n",
       "      <td>903.194734</td>\n",
       "      <td>1.003537</td>\n",
       "      <td>0.331115</td>\n",
       "      <td>0.679910</td>\n",
       "      <td>1.538540</td>\n",
       "      <td>1.498737</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>921.023640</td>\n",
       "      <td>3.723706</td>\n",
       "      <td>2.145861</td>\n",
       "      <td>2.172811</td>\n",
       "      <td>1.609741</td>\n",
       "      <td>0.342405</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>969.817274</td>\n",
       "      <td>1.519595</td>\n",
       "      <td>1.152461</td>\n",
       "      <td>1.794775</td>\n",
       "      <td>2.638535</td>\n",
       "      <td>-0.537375</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>84</td>\n",
       "      <td>13</td>\n",
       "      <td>922.297143</td>\n",
       "      <td>-0.142724</td>\n",
       "      <td>1.691849</td>\n",
       "      <td>-0.190819</td>\n",
       "      <td>1.448809</td>\n",
       "      <td>2.601015</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>76</td>\n",
       "      <td>5</td>\n",
       "      <td>940.757064</td>\n",
       "      <td>1.043017</td>\n",
       "      <td>-0.384232</td>\n",
       "      <td>0.513526</td>\n",
       "      <td>0.139178</td>\n",
       "      <td>1.566563</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>298.065958</td>\n",
       "      <td>0.112356</td>\n",
       "      <td>-0.549161</td>\n",
       "      <td>1.162953</td>\n",
       "      <td>-0.837808</td>\n",
       "      <td>0.725265</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>96</td>\n",
       "      <td>4</td>\n",
       "      <td>951.047558</td>\n",
       "      <td>1.211395</td>\n",
       "      <td>1.360046</td>\n",
       "      <td>2.542116</td>\n",
       "      <td>0.445173</td>\n",
       "      <td>1.289715</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>339.098289</td>\n",
       "      <td>0.420844</td>\n",
       "      <td>1.595147</td>\n",
       "      <td>0.521304</td>\n",
       "      <td>3.332927</td>\n",
       "      <td>1.128712</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>45</td>\n",
       "      <td>22</td>\n",
       "      <td>309.231187</td>\n",
       "      <td>0.884177</td>\n",
       "      <td>1.273291</td>\n",
       "      <td>0.203366</td>\n",
       "      <td>0.645169</td>\n",
       "      <td>1.318675</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>61.868779</td>\n",
       "      <td>2.075375</td>\n",
       "      <td>1.438367</td>\n",
       "      <td>0.485358</td>\n",
       "      <td>2.523798</td>\n",
       "      <td>-0.259758</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "      <td>116.987508</td>\n",
       "      <td>1.571186</td>\n",
       "      <td>0.233845</td>\n",
       "      <td>1.420047</td>\n",
       "      <td>1.223781</td>\n",
       "      <td>0.972551</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>63</td>\n",
       "      <td>4</td>\n",
       "      <td>951.047558</td>\n",
       "      <td>1.072359</td>\n",
       "      <td>1.687623</td>\n",
       "      <td>2.542116</td>\n",
       "      <td>0.445173</td>\n",
       "      <td>2.425623</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>99</td>\n",
       "      <td>17</td>\n",
       "      <td>858.468796</td>\n",
       "      <td>1.403832</td>\n",
       "      <td>2.795258</td>\n",
       "      <td>1.926593</td>\n",
       "      <td>0.963505</td>\n",
       "      <td>1.500326</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>410.171349</td>\n",
       "      <td>-0.050523</td>\n",
       "      <td>2.409485</td>\n",
       "      <td>1.983686</td>\n",
       "      <td>0.490864</td>\n",
       "      <td>0.183733</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>672.663258</td>\n",
       "      <td>1.519595</td>\n",
       "      <td>1.152461</td>\n",
       "      <td>1.190294</td>\n",
       "      <td>0.411279</td>\n",
       "      <td>1.779912</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>409.013301</td>\n",
       "      <td>0.148511</td>\n",
       "      <td>-0.805802</td>\n",
       "      <td>1.453754</td>\n",
       "      <td>1.895166</td>\n",
       "      <td>0.864363</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  item_id       price         0         1         2         3  \\\n",
       "0         77       28   28.698722  0.789939  2.507614  0.900374  1.040794   \n",
       "1         33       26  248.907550 -1.330941  1.305624  1.427514  0.968023   \n",
       "2         76        8  367.506014  1.043017 -0.384232  0.371040  1.340897   \n",
       "3         31       15   61.868779  0.400431  0.665219  0.485358  2.523798   \n",
       "4         72        7  969.817274 -1.079973  1.119112  1.794775  2.638535   \n",
       "5         68       28   28.698722  1.196186  0.559321  0.900374  1.040794   \n",
       "6         90       28   28.698722 -1.912106  1.681800  0.900374  1.040794   \n",
       "7         74       14  903.194734  1.562109  2.590898  0.679910  1.538540   \n",
       "8         50       25  993.689301  0.832800  1.486144  1.065739  1.375797   \n",
       "9         19        7  969.817274  1.732512  0.016619  1.794775  2.638535   \n",
       "10        33        7  969.817274 -1.330941  1.305624  1.794775  2.638535   \n",
       "11        18       10   17.827325  1.394354  2.027665 -0.453177  0.125841   \n",
       "12        68       29  254.746559  1.196186  0.559321  0.772554  2.273993   \n",
       "13         0       10   17.827325  1.519595  1.152461 -0.453177  0.125841   \n",
       "14        84        0  100.947533 -0.142724  1.691849  1.325641  2.517639   \n",
       "15        38       21  835.214987  0.386115  1.626517  0.731811  0.382787   \n",
       "16        14        0  100.947533  3.723706  2.145861  1.325641  2.517639   \n",
       "17        94       13  922.297143  1.729603  1.883731 -0.190819  1.448809   \n",
       "18        49        1  410.171349  2.404322  2.625245  1.983686  0.490864   \n",
       "19        87       23  672.663258  2.584417 -0.124552  1.190294  0.411279   \n",
       "20        54        3   71.688406 -0.065181 -0.448748  1.051414  0.777089   \n",
       "21        61        0  100.947533  0.581359  1.570981  1.325641  2.517639   \n",
       "22        85       12  531.135325 -1.446959  0.728564  1.759724  0.253640   \n",
       "23        61       28   28.698722  0.581359  1.570981  0.900374  1.040794   \n",
       "24        29        2  116.987508  0.856086 -0.159930  1.420047  1.223781   \n",
       "25        54        8  367.506014 -0.065181 -0.448748  0.371040  1.340897   \n",
       "26        70       25  993.689301  1.415843  0.681872  1.065739  1.375797   \n",
       "27        13       29  254.746559 -0.607144  0.920339  0.772554  2.273993   \n",
       "28        84       28   28.698722 -0.142724  1.691849  0.900374  1.040794   \n",
       "29        33        4  951.047558 -1.330941  1.305624  2.542116  0.445173   \n",
       "..       ...      ...         ...       ...       ...       ...       ...   \n",
       "170       18        2  116.987508  1.394354  2.027665  1.420047  1.223781   \n",
       "171        5       23  672.663258  0.443231  0.812970  1.190294  0.411279   \n",
       "172       85       18  673.090628 -1.446959  0.728564  1.519853  0.484322   \n",
       "173       25        9   43.473372  0.649671  1.368671  2.454132 -0.709874   \n",
       "174       19       18  673.090628  1.732512  0.016619  1.519853  0.484322   \n",
       "175       71       26  248.907550 -0.086325  1.039805  1.427514  0.968023   \n",
       "176       83       10   17.827325  2.014021  0.240536 -0.453177  0.125841   \n",
       "177       87        5  940.757064  2.584417 -0.124552  0.513526  0.139178   \n",
       "178       92       11  339.098289  0.575016  1.578828  0.521304  3.332927   \n",
       "179       70       23  672.663258  1.415843  0.681872  1.190294  0.411279   \n",
       "180       39       28   28.698722 -0.593363  2.374605  0.900374  1.040794   \n",
       "181       42        0  100.947533 -0.677713  1.405754  1.325641  2.517639   \n",
       "182       68        3   71.688406  1.196186  0.559321  1.051414  0.777089   \n",
       "183       53       18  673.090628  1.497579  0.342245  1.519853  0.484322   \n",
       "184       62       14  903.194734  1.003537  0.331115  0.679910  1.538540   \n",
       "185       14       16  921.023640  3.723706  2.145861  2.172811  1.609741   \n",
       "186        0        7  969.817274  1.519595  1.152461  1.794775  2.638535   \n",
       "187       84       13  922.297143 -0.142724  1.691849 -0.190819  1.448809   \n",
       "188       76        5  940.757064  1.043017 -0.384232  0.513526  0.139178   \n",
       "189       21       27  298.065958  0.112356 -0.549161  1.162953 -0.837808   \n",
       "190       96        4  951.047558  1.211395  1.360046  2.542116  0.445173   \n",
       "191       27       11  339.098289  0.420844  1.595147  0.521304  3.332927   \n",
       "192       45       22  309.231187  0.884177  1.273291  0.203366  0.645169   \n",
       "193       20       15   61.868779  2.075375  1.438367  0.485358  2.523798   \n",
       "194       78        2  116.987508  1.571186  0.233845  1.420047  1.223781   \n",
       "195       63        4  951.047558  1.072359  1.687623  2.542116  0.445173   \n",
       "196       99       17  858.468796  1.403832  2.795258  1.926593  0.963505   \n",
       "197       10        1  410.171349 -0.050523  2.409485  1.983686  0.490864   \n",
       "198        0       23  672.663258  1.519595  1.152461  1.190294  0.411279   \n",
       "199       16       24  409.013301  0.148511 -0.805802  1.453754  1.895166   \n",
       "\n",
       "            4  action  \n",
       "0    2.902107       0  \n",
       "1    0.186144       1  \n",
       "2    1.158571       2  \n",
       "3   -0.459346       3  \n",
       "4    0.742932       4  \n",
       "5    2.364322       5  \n",
       "6    1.542127       6  \n",
       "7    0.280190       7  \n",
       "8    0.177736       8  \n",
       "9    1.700727       9  \n",
       "10  -0.002220      10  \n",
       "11   0.835739      11  \n",
       "12  -0.347443      12  \n",
       "13   2.242180      13  \n",
       "14   0.860762      14  \n",
       "15   0.234187      15  \n",
       "16   0.784364      16  \n",
       "17   1.220363      17  \n",
       "18   1.008308      18  \n",
       "19   1.683473      19  \n",
       "20  -0.956457      20  \n",
       "21  -0.313141      21  \n",
       "22  -0.335128      22  \n",
       "23   1.114783      23  \n",
       "24   3.580429      24  \n",
       "25   0.057889      25  \n",
       "26   1.716005      26  \n",
       "27   0.229921      27  \n",
       "28   0.159544      28  \n",
       "29  -1.186362      29  \n",
       "..        ...     ...  \n",
       "170  1.180536     170  \n",
       "171  0.193846     171  \n",
       "172 -0.271625     172  \n",
       "173  0.677057     173  \n",
       "174  0.685074     174  \n",
       "175  1.887842     175  \n",
       "176  1.113439     176  \n",
       "177 -0.547325     177  \n",
       "178  1.844110     178  \n",
       "179  2.056615     179  \n",
       "180  2.199441     180  \n",
       "181 -0.736620     181  \n",
       "182  1.333610     182  \n",
       "183  1.191190     183  \n",
       "184  1.498737     184  \n",
       "185  0.342405     185  \n",
       "186 -0.537375     186  \n",
       "187  2.601015     187  \n",
       "188  1.566563     188  \n",
       "189  0.725265     189  \n",
       "190  1.289715     190  \n",
       "191  1.128712     191  \n",
       "192  1.318675     192  \n",
       "193 -0.259758     193  \n",
       "194  0.972551     194  \n",
       "195  2.425623     195  \n",
       "196  1.500326     196  \n",
       "197  0.183733     197  \n",
       "198  1.779912     198  \n",
       "199  0.864363     199  \n",
       "\n",
       "[200 rows x 9 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_data = pd.DataFrame(action_history, columns=['actions'])\n",
    "\n",
    "pos_users = []\n",
    "pos_items = []\n",
    "pos_meta = []\n",
    "\n",
    "for i in range(len(action_history)):\n",
    "    state = state_history[i]\n",
    "    action = action_history[i]\n",
    "    \n",
    "    pos_users.append(state[0][0])\n",
    "    pos_items.append(state[action][1])\n",
    "    pos_meta.append(state[action][2:])\n",
    "    \n",
    "    \n",
    "pos_data['user_id'] = pos_users\n",
    "pos_data['item_id'] = pos_items\n",
    "pos_data[['price', '0', '1', '2', '3', '4']] = pd.DataFrame(pos_meta)\n",
    "\n",
    "\n",
    "pos_data = pos_data.drop('actions', axis=1)\n",
    "\n",
    "pos_data['action'] = pos_data.reset_index()['index']\n",
    "\n",
    "pos_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_other_random(row, rng):\n",
    "    neg_id = row['item_id']\n",
    "    action = row['action']\n",
    "    ids_list = np.array(state_history[int(action)])[:, 1]\n",
    "    while neg_id == row['item_id']:\n",
    "        neg_id = rng.choice(ids_list)\n",
    "    return int(neg_id)\n",
    "\n",
    "def get_neg_meta(row):\n",
    "    a = int(row['action'])\n",
    "    i = int(row['neg_id'])\n",
    "    ids_list = np.array(state_history[a])[:, 1]\n",
    "    indice = np.where(ids_list == i)[0][0]\n",
    "    return state_history[a][indice][2:]\n",
    "\n",
    "\n",
    "def sample_train(pos_data, random_seed=0):\n",
    "    \"\"\"Sample negatives at random\"\"\"\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    \n",
    "    train_data = pd.DataFrame(pos_data)\n",
    "    train_data['neg_id'] = train_data.apply(lambda row: find_other_random(row, rng), axis=1)\n",
    "    \n",
    "    train_data[['neg_price', 'neg_0', 'neg_1', 'neg_2', 'neg_3', 'neg_4']] = train_data.apply(lambda row: get_neg_meta(row), result_type='expand', axis=1)\n",
    "    \n",
    "    \n",
    "    return train_data\n",
    "\n",
    "#sample_train(pos_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Embedding, Flatten, Input, Dense, Dropout\n",
    "from keras.layers import Concatenate, Lambda\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_loss(y_true, y_pred):\n",
    "    \"\"\"Ignore y_true and return the mean of y_pred\n",
    "    \n",
    "    This is a hack to work-around the design of the Keras API that is\n",
    "    not really suited to train networks with a triplet loss by default.\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(y_pred + 0 * y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_comparator_loss_improved(inputs, loss_type=\"improved2\", A=0.01, B=0.):\n",
    "    \"\"\"Comparator loss for a pair of precomputed similarities\n",
    "    \n",
    "    If the inputs are cosine similarities, they each have range in\n",
    "    (-1, 1), therefore their difference have range in (-2, 2). Using\n",
    "    a margin of 1. can therefore make sense.\n",
    "\n",
    "    If the input similarities are not normalized, it can be beneficial\n",
    "    to use larger values for the margin of the comparator loss.\n",
    "    \"\"\"\n",
    "    p_sim, n_sim, p_metadata, n_metadata = inputs\n",
    "    p_price = p_metadata[0]\n",
    "    n_price = n_metadata[0]\n",
    "    \n",
    "    # The basic version of the loss, whithout considering the price.\n",
    "    if loss_type=='basic':\n",
    "        return tf.maximum(n_sim - p_sim + 1, 0)\n",
    "  \n",
    "    if p_sim == n_sim:\n",
    "        return tf.maximum((n_price - p_price + B), 0)\n",
    "    \n",
    "    # The first version of the improved loss\n",
    "    if loss_type==\"improved1\":\n",
    "        return tf.maximum((A / tf.abs(n_sim - p_sim + 1)) * (n_price - p_price + B) + (n_sim - p_sim), 0)\n",
    "    \n",
    "    # A second version of the improved loss\n",
    "    return tf.maximum((-0.5*tf.abs(n_sim - p_sim)+2) * (5/100) * (n_price - p_price) + (n_sim - p_sim + 1 ), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_interaction_mlp(input_dim, n_hidden=1, hidden_size=64,\n",
    "                         dropout=0, l2_reg=None):\n",
    "    \"\"\"Build the shared multi layer perceptron\"\"\"\n",
    "    mlp = Sequential()\n",
    "    if n_hidden == 0:\n",
    "        # Plug the output unit directly: this is a simple\n",
    "        # linear regression model. Not dropout required.\n",
    "        mlp.add(Dense(1, input_dim=input_dim,\n",
    "                      activation='sigmoid', kernel_regularizer=l2_reg))\n",
    "    else:\n",
    "        mlp.add(Dense(hidden_size, input_dim=input_dim,\n",
    "                      activation='sigmoid', kernel_regularizer=l2_reg))\n",
    "        mlp.add(Dropout(dropout))\n",
    "        for i in range(n_hidden - 1):\n",
    "            mlp.add(Dense(hidden_size, activation='sigmoid',\n",
    "                          W_regularizer=l2_reg))\n",
    "            mlp.add(Dropout(dropout))\n",
    "        mlp.add(Dense(1, activation='sigmoid', kernel_regularizer=l2_reg))\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_models(n_users, n_items, n_meta=6, user_dim=5, item_dim=5,\n",
    "                 n_hidden=1, hidden_size=64, dropout=0, loss_type=\"improved2\", l2_reg=0):\n",
    "    \"\"\"Build models to train a deep triplet network\"\"\"\n",
    "    \n",
    "    user_input = Input((1,), name='user_input')\n",
    "    positive_item_input = Input((1,), name='positive_item_input')\n",
    "    positive_meta_data_input = Input((n_meta,), name='positive_meta_data_input')\n",
    "    negative_item_input = Input((1,), name='negative_item_input')\n",
    "    negative_meta_data_input = Input((n_meta,), name='negative_meta_data_input')\n",
    "    \n",
    "    l2_reg = None if l2_reg == 0 else l2(l2_reg)\n",
    "    \n",
    "    user_layer = Embedding(n_users, user_dim, input_length=1,\n",
    "                           name='user_embedding', embeddings_regularizer=l2_reg)\n",
    "\n",
    "    # The following embedding parameters will be shared to encode both\n",
    "    # the positive and negative items.\n",
    "    item_layer = Embedding(n_items, item_dim, input_length=1,\n",
    "                           name=\"item_embedding\", embeddings_regularizer=l2_reg)\n",
    "\n",
    "    #user_embedding = Flatten()(user_layer(user_input))\n",
    "    #positive_item_embedding = Flatten()(item_layer(positive_item_input)) \n",
    "    #negative_item_embedding = Flatten()(item_layer(negative_item_input))\n",
    "   \n",
    "\n",
    "\n",
    "    # Similarity computation between embeddings using a MLP similarity\n",
    "    positive_embeddings_pair = Concatenate(name=\"positive_embeddings_pair\")(\n",
    "        [user_input, positive_item_input, positive_meta_data_input])\n",
    "    positive_embeddings_pair = Dropout(dropout)(positive_embeddings_pair)\n",
    "    negative_embeddings_pair = Concatenate(name=\"negative_embeddings_pair\")(\n",
    "        [user_input, negative_item_input, negative_meta_data_input])\n",
    "    negative_embeddings_pair = Dropout(dropout)(negative_embeddings_pair)\n",
    "\n",
    "    # Instanciate the shared similarity architecture\n",
    "    interaction_layers = make_interaction_mlp(\n",
    "        1 + 1 + n_meta, n_hidden=n_hidden, hidden_size=hidden_size,\n",
    "        dropout=dropout)\n",
    "    \n",
    "\n",
    "    positive_similarity = interaction_layers(positive_embeddings_pair)\n",
    "    negative_similarity = interaction_layers(negative_embeddings_pair)\n",
    "\n",
    "    # The triplet network model, only used for training\n",
    "    \n",
    "    triplet_loss = Lambda(\n",
    "        margin_comparator_loss_improved,\n",
    "        arguments={'loss_type':loss_type},\n",
    "        output_shape=(1,),\n",
    "        name='comparator_loss')(\n",
    "            [positive_similarity,\n",
    "             negative_similarity,\n",
    "             positive_meta_data_input,\n",
    "             negative_meta_data_input]\n",
    "        )\n",
    "\n",
    "    deep_triplet_model = Model(inputs=[user_input,\n",
    "                                       positive_item_input,\n",
    "                                       positive_meta_data_input,\n",
    "                                       negative_item_input,\n",
    "                                       negative_meta_data_input],\n",
    "                               outputs=[triplet_loss])\n",
    "\n",
    "    \n",
    "    # The match-score model, only used at inference\n",
    "    deep_match_model = Model(inputs=[user_input, positive_item_input, positive_meta_data_input],\n",
    "                             outputs=[positive_similarity])\n",
    "\n",
    "    return deep_match_model, deep_triplet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_input (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive_item_input (InputLayer (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive_meta_data_input (Input (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative_item_input (InputLayer (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative_meta_data_input (Input (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive_embeddings_pair (Conca (None, 8)            0           user_input[0][0]                 \n",
      "                                                                 positive_item_input[0][0]        \n",
      "                                                                 positive_meta_data_input[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "negative_embeddings_pair (Conca (None, 8)            0           user_input[0][0]                 \n",
      "                                                                 negative_item_input[0][0]        \n",
      "                                                                 negative_meta_data_input[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_87 (Dropout)            (None, 8)            0           positive_embeddings_pair[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_88 (Dropout)            (None, 8)            0           negative_embeddings_pair[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "sequential_33 (Sequential)      (None, 1)            9           dropout_87[0][0]                 \n",
      "                                                                 dropout_88[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "comparator_loss (Lambda)        (None, 1)            0           sequential_33[1][0]              \n",
      "                                                                 sequential_33[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 9\n",
      "Trainable params: 9\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "deep_triplet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/.local/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, activation=\"sigmoid\", kernel_regularizer=None)`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "200/200 [==============================] - 6s 31ms/step - loss: 1.5903\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 128us/step - loss: 4.0431\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 129us/step - loss: 3.7254\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 162us/step - loss: 5.4763\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 174us/step - loss: 1.2469\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 154us/step - loss: 3.7718\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 132us/step - loss: 1.4841\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 143us/step - loss: 6.2152\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 167us/step - loss: 5.1656\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 145us/step - loss: 4.2123\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 106us/step - loss: 3.8834\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 158us/step - loss: 2.4852\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 150us/step - loss: 11.3769\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 228us/step - loss: 2.8542\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 137us/step - loss: 5.6956\n"
     ]
    }
   ],
   "source": [
    "hyper_parameters = dict(\n",
    "    n_meta=6,\n",
    "    user_dim=1,\n",
    "    item_dim=1,\n",
    "    n_hidden=2,\n",
    "    hidden_size=128,\n",
    "    dropout=0.1,\n",
    "    loss_type=\"improved22\",\n",
    "    l2_reg=0\n",
    ")\n",
    "deep_match_model, deep_triplet_model = build_models(nb_users, nb_items,\n",
    "                                                    **hyper_parameters)\n",
    "\n",
    "\n",
    "deep_triplet_model.compile(loss=identity_loss, optimizer='adam')\n",
    "fake_y = np.ones_like(pos_data['user_id'])\n",
    "\n",
    "n_epochs = 15\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    # Sample new negatives to build different triplets at each epoch\n",
    "    train_inputs = sample_train(pos_data, random_seed=i)\n",
    "\n",
    "    # Fit the model incrementally by doing a single pass over the\n",
    "    # sampled triplets.\n",
    "    deep_triplet_model.fit(\n",
    "        [\n",
    "            train_inputs['user_id'].values,\n",
    "            train_inputs['item_id'].values,\n",
    "            train_inputs[['price', '0', '1', '2', '3','4']].values,\n",
    "            train_inputs['neg_id'].values,\n",
    "            train_inputs[['neg_price', 'neg_0', 'neg_1', 'neg_2', 'neg_3','neg_4']].values\n",
    "        ],\n",
    "        fake_y,\n",
    "        epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative reward: 22110.680\n",
      "Cumulative reward: 26536.714\n",
      "Cumulative reward: 23250.867\n",
      "\n",
      "Average cumulative reward: 23966.087\n"
     ]
    }
   ],
   "source": [
    "nb_episodes = 100\n",
    "cum_rewards=[]\n",
    "\n",
    "for j in range(3):\n",
    "    cum_reward = 0\n",
    "    for i in range(nb_episodes):\n",
    "        next_state = np.array(next_state)\n",
    "        predicted = int(np.argmax(deep_match_model.predict([next_state[:,0], next_state[:,1], next_state[:,2:]])))\n",
    "        r = requests.get(url=env+'predict', params= {'user_id':USER_ID, 'recommended_item': predicted})\n",
    "        next_state = r.json()['state']\n",
    "        reward = r.json()['reward']\n",
    "        cum_reward += reward \n",
    "        #print('Predicted: {:d}, reward: {:.3f}'.format(predicted, reward))\n",
    "    \n",
    "    print('Cumulative reward: {:.3f}'.format(cum_reward))\n",
    "    cum_rewards.append(cum_reward)\n",
    "    \n",
    "\n",
    "print('\\nAverage cumulative reward: {:.3f}'.format(np.mean(cum_rewards)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
